---
title: "Reflection"
author: "Tahmid Ahmed"
date: "11/23/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r data, echo=FALSE, message=FALSE, warning=FALSE}
# Load libraries
library(tidyverse)
library(caret)
library(statebins)
library(usmap)
library(stargazer)
library(cowplot)
library(sjPlot)
library(magrittr)
library(scales)
library(performance)
library(gt)
library(plotly)
library(grid)
library(gridExtra)
```

### Overview

Now that the 2020 election is officially over and Biden was elected as the President of the United States, it is important that I reflect on my prediction model. I am excited to see how I cold learn from my model for future models that I create. 

### Model Recap and Predictions

Let's first recap on my prediction model to get a better picture of what it was. 

* My prediction model was an **ensemble model** that predicted the popular vote share for each state
.
* Given that the Time For Change Model was an inspiration, I decided to focus my model on historical republican vote share as Trump was the incumbent for the 2020 election and incumbency was one predictor in the Time For Change Model.

* I decided to separate America into three categories - red states, blue states, and battleground states - for my model to adjust for overfitting. The grouping were based on how FiveThirtyEight grouped states. 
  + Red States - AK, IN, KS, MO, AL, AR, ID, KY, LA, MS, ND, OK, SD, MT, TN, WV, WY, SC, UT, NE
  + Blue States - CO, VA, CA, CT, DE, HI, IL, MD, MA, NJ, NY, OR, RI, VT, WA, ME, NM, NH
  + Battleground states - FL, IA, OH, GA, NC, MI, MN, PA, WI, NV, AZ, TX
  
* My model used the following data:
  + Historical polling, approval, and turnout data was based on data given from class
  + Historical and present economic data was sourced from the Bureau of Economic Analysis
  + Present polling and approval data was based on FiveThirtyEightâ€™s forecast data
  
* In my model, I decided to classify approval, Q2 GDP growth, and turnout as fundamentals

* Thus, **my ensemble model weighted the poll model (using only polls) by 0.96 and the fundamental model (using only fundamentals) by 0.04** as I weighted the model based on FiveThirtyEight's reasoning that polls are better predictors as the election nears since fundamentals become more noisy instead.
  + Trump vote share = 0.96(Poll) + 0.04(Fundamental)


* My final prediction using the ensemble model was that **Biden was projected to win 310 electoral votes while Trump is projected to win 228 votes**, meaning Biden would become president-elect of the United States.

### Patterns and Accuracy

Overall, I am pretty satisfied with how my model turned out. While I did miss a few states and this is my first election forecast, I was quite happy that I predicted some battleground states correctly.

```{r join,echo=FALSE, message=FALSE, warning=FALSE}
# Read data needed
myprediction <- read_csv("/Users/tahmidahmed/Desktop/Gov 1347/Gov1347/data/pred_ensemble.csv")
popvote <- read_csv("/Users/tahmidahmed/Desktop/Gov 1347/Gov1347/data/popvote_bystate_1948-2020.csv") %>%
  filter(year == 2020)
popvote$state <- state.abb[match(popvote$state, state.name)] 
popvote$R_pv2p <- popvote$R_pv2p * 100

# Join data
joined <- myprediction %>%
  left_join(popvote, by = "state") %>%
  mutate(winner_pv = ifelse(R_pv2p > 50, "Republican", "Democrat")) %>%
  mutate(diff = R_pv2p - pred)
```

```{r, echo=FALSE, warning=FALSE, fig.height = 3, fig.width = 10}
# Plot results of ensemble model
pred_map <- plot_usmap(data = joined, regions = "states", values = "winner") +
  scale_fill_manual(breaks = c("Democrat", "Republican"),
                    values = c(muted("blue"), "red3")) +
  theme_void() +
  labs(fill = "Political Party",
       title = "2020 Presidential Election Prediction Map",
       subtitle = "Weighting = 0.96*Poll + 0.04*Fundamental")

# Plot results of actual results
actual_map <- plot_usmap(data = joined, regions = "states", values = "winner_pv") +
  scale_fill_manual(breaks = c("Democrat", "Republican"),
                    values = c(muted("blue"), "red3")) +
  theme_void() +
  labs(fill = "Political Party",
       title = "2020 Presidential Election Actual Map",
       subtitle = "Live Update")

grid.arrange(pred_map, actual_map, ncol = 2)

```

Above is a comparison between my predictions and the actual results of the 2020 election. As you can see, the states that I got wrong were battleground states. However, I would like to say that the predictive intervals for the battleground states did capture the true result. 

+ The states that **I predicted incorrectly were Arizona, Nebraska, Florida, Wisconsin, Georgia, and Ohio**.
+ This means that my **Classification Accuracy is 88%**. 

```{r, echo=FALSE}
# Calculating the mse and rmse for all states
mse_all = sum(joined$diff**2) / nrow(joined)
rmse_all = sqrt(mse_all)

# correlation between actual and predicted
corr <- joined %>% 
  select(R_pv2p, pred) %>% 
  cor() %>% 
  as_tibble() %>% 
  slice(1) %>% 
  pull(2)
```

```{r joinedplot, echo=FALSE, warning=FALSE}
# Plotting actual vs predicted state pv2p
joined_plot <- joined %>% 
  ggplot(aes(x = pred, y = R_pv2p, color = winner_pv, labels = state)) +
  geom_point() + 
  geom_abline() +
  scale_color_manual(values = c(muted("blue"), "red3")) +
  scale_x_continuous(labels = percent_format(accuracy = 1, scale = 1), limits = c(25, 95), breaks = c(30, 40, 50, 60, 70, 80, 90)) +
  scale_y_continuous(labels = percent_format(accuracy = 1, scale = 1), limits = c(25, 95), breaks = c(30, 40, 50, 60, 70, 80, 90)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(title = "Actual vs Predicted Trump Two-Party Popular Vote per State",
       x = "Predicted Trump Two-Party Popular Vote",
       y = "Actual Trump Two-Party Popular Vote")

# Switching to plotly
ggplotly(joined_plot, tooltip = c("state", "R_pv2p", "pred"))
```

Moreover, let's take a look into the plot above, which plots the actual two-party vote share for Trump against my predictions for Trump. The blue points represent states Biden won and the red points represent states Trump won.

+ The **correlation between actual and predicted two-party vote share for each state is 0.88**, which is fairly strong.
+ The **root mean squared error of all my state predictions is approximately 5.02 percentage points**. While this isn't too bad, some states were very off from my predictions. 
+ It is interesting to note that there is a fair share of both overpredicting and underpredicting Trump's vote shares in each state, evidenced by the plot above.


```{r, echo=FALSE, warning=FALSE}
# Plot forecast error
joined$hover <- with(joined, paste(state, '<br>'))

# give state boundaries a white border
l <- list(color = toRGB("white"), width = 2)
# specify some map projection/options
g <- list(
  scope = 'usa',
  projection = list(type = 'albers usa'),
  showlakes = TRUE,
  lakecolor = toRGB('white')
)

plot_geo(joined, locationmode = 'USA-states') %>%
  add_trace(
  z = ~diff, text = ~hover, locations = ~state,
  color = ~diff, colors = 'Purples') %>%
  layout(
    title = "Difference between Trump's actual and predicted two party vote share <br>(Hover for breakdown)",
    geo = g) %>% colorbar(title = "Difference")
```

Furthermore, the map above shows the difference between Trump's actual and predicted two party vote share in each state. A negative difference means that Trump was overpredicted for that particular state while a positive difference means that Trump was underpredicted for that particular state. 

* It is interesting to note that states where Trump was greatly overpredicted or greatly underpredicted are states that are not battleground states. This makes sense because states that are traditionally red or blue and not battleground typically have less polling as there is a small chance that those states will flip. This is why we may see a state like Alaska with little polling where Trump be greatly overpredicted there.  

```{r, echo=FALSE}
# generating histograms
ggplotly(
joined %>% 
    ggplot(aes(diff)) +
    geom_histogram(aes(y = after_stat(count / sum(count))), bins = 10, fill = '#CD5C5C') + 
    geom_vline(xintercept = 0, linetype = "dashed") +
    labs(x = "Difference Between Trump's Actual and Predicted \nTwo-Party Vote Share",
         y = "Count",
         title = "Error Distribution for Model") +
    theme_classic())
```

* The above histogram shows the error distribution for my prediction model and the model's average error seems to be mainly  normally distributed around 0. 

### Hypotheses for why my model was inaccurate

Now that we have went over my prediction model, it is important to look at possible hypotheses for the inaccuracies seen in my model. My model seemed to incorrectly predict the results for battleground states in particular and it is important we pay attention to the reasons why. Below are my hypotheses for explaining the inaccuracies of my model:

* One hypothesis to explain the inaccuracy of my model was that it failed to take into account the recent voter trends in particular states. For example, Georgia and Texas have been trending blue recently but my model failed to take note of this. This could be because my model relied more heavily on historical polling averages and so since Georgia and Texas were traditionally red states, my model would predict the same for 2020.
  + Moreover, my model failed to consider the recent voter trends in populous counties. For example, Miami-Dade County saw became much more red than 2016 and heavily helped Trump win Florida again. Likewise, Fulton county in Georgia heavily favored Joe Biden in 2020, which played a significant role in turning Georgia blue. Thus, it is important that prediction models take into consideration trends not just in states but in counties as well since some counties alone can determine the overall result for the state it is in. 
  
* While my model took into consideration the expected increase in turnout rate for the 2020 election, my model failed to take into consideration the turnout rates for different groups. For example, Stacy Abrams played a crucial role in black voter-turnout in Georgia in favor for Biden. The same goes for the large Latinx turnout in Arizona and Nevada, which also helped Biden. However, there were many Latinos that voted for Trump particularly in South Texas and Florida. Given the large turnout rates for some of these groups, they can play a significant role in determining the election. 

* Another hypothesis is that my model relied heavily on inaccurate polls. Some polls in 2020 were fairly [inaccurate](https://www.scientificamerican.com/article/why-polls-were-mostly-wrong/) because they were non-representative of voters and there was non-response bias, particularly from conservatives. On average, polls were off by 2.5 points in battleground states. Given the inaccuracy of polls, this may explain why my model had inaccuracies, especially since I weighted the poll model by 0.96 in my ensemble model. 

+ Another hypothesis is that the state Q2 GDP growth rate as a fundamental variable may have hurt Trump more than it was supposed to, especially in battleground states and traditionally red states. This is because economic predictors were very noisy this year due to a recession caused by Trump's handling of the Covid pandemic. Since the 2020 economy was an anomaly, it probably would be best to not use economic predictors in my model. 























